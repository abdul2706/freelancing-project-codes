# -*- coding: utf-8 -*-
"""fiverr-tensorflow-mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tvOqxJNBzJcjvMujJCq-jtlvLrtjNz5M

# MNIST Hand Digit Recognition using Tensorflow

Connect Google Drive for saving trained models
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Import Libraries"""

import os
import cv2
import time
import numpy as np
from matplotlib import pyplot as plt

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization, Dropout, ReLU
from tensorflow.keras.metrics import Accuracy, Precision, Recall
from tensorflow.keras.backend import clear_session
from tensorflow.keras.datasets import mnist
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.applications import vgg16
from tensorflow.keras.optimizers import Adam, SGD

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print(tf.config.list_physical_devices('GPU'))

"""# Load Dataset"""

BASE_PATH = '/content/drive/My Drive/fiverr/fyzon_36/2022-05-17-tensorflow-mnist'

# load dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# reshape data to have a single channel
X_train = X_train[..., np.newaxis]
X_test = X_test[..., np.newaxis]

# determine the shape of the input images
in_shape = X_train.shape[1:]
print('in_shape:', in_shape)

# determine the number of classes
n_classes = len(np.unique(y_train))
print('n_classes:', n_classes)

# normalize pixel values
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# view size of train and test sets
print('X_train:', X_train.shape)
print('y_train:', y_train.shape)
print('X_test:', X_test.shape)
print('y_test:', y_test.shape)

"""# Create Custom Model"""

def create_model():
    # Creating the model
    model = Sequential()
    model.add(Conv2D(32, (3, 3), padding="same", input_shape=in_shape))
    model.add(BatchNormalization())
    model.add(ReLU())
    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

    model.add(Conv2D(64, (3, 3), padding="same"))
    model.add(BatchNormalization())
    model.add(ReLU())
    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))
    model.add(Dropout(0.3))

    model.add(Conv2D(128, (3, 3), padding="same"))
    model.add(BatchNormalization())
    model.add(ReLU())
    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))
    model.add(Dense(128, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout(0.3))
    model.add(Dense(n_classes, activation='softmax'))
    
    return model

# clear any tensorflow session to delete models in memory (if any)
clear_session()
# create model object
model = create_model()
# displaying the model in text form
model.summary()
# displaying the model in plot form
plot_model_path = os.path.join(BASE_PATH, 'model-summary.png')
plot_model(model, plot_model_path, show_shapes=True)

"""# Train Models for 2 learning algorithms and 3 learning rates"""

def train_model(model, optimizer_name, learning_rate, X_train, y_train, X_test, y_test):
    # python dictionary to map optimizer_name arguemnt to actual optimizer class
    optimizer_map = {'sgd': SGD, 'adam': Adam}
    # get actual optimizer class from optimizer_name argument
    optimizer_class = optimizer_map[optimizer_name]
    # create tensorflow optimizer object
    optimizer = optimizer_class(learning_rate=learning_rate)

    # define loss and optimizer
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # configure early stopping
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)

    # fit the model
    history = model.fit(X_train, y_train, epochs=50, batch_size=1024, verbose=1, validation_split=0.2, callbacks=[early_stopping])

    # evaluate the model
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)

    # save model to file
    model_save_path = os.path.join(BASE_PATH, f'model-lr_{learning_rate}-optim_{optimizer_name}.h5')
    model.save(model_save_path)

    return history, test_loss, test_acc

optimizers = ['sgd', 'adam']
learning_rates = [0.1, 0.01, 0.001]

# clear any tensorflow session to delete models in memory (if any)
clear_session()

# list to save training related data for each optimizer and learning_rate combination
trained_models_data = []

for optimizer_name in optimizers:
    for learning_rate in learning_rates:
        # create tensorflow model object
        model = create_model()
        history, test_loss, test_acc = train_model(model, optimizer_name, learning_rate, X_train, y_train, X_test, y_test)
        trained_models_data.append([optimizer_name, learning_rate, model, history, test_loss, test_acc])
        print(f'optim={optimizer_name}, lr={learning_rate}, test_acc={test_acc:.4f}, test_loss={test_loss:.4f}')
        print()

print('total trained models:', len(trained_models_data))

"""# Plot Learning/Training Curves"""

fig, axarr = plt.subplots(nrows=2, ncols=2, figsize=(16.0, 12.0))
fig.suptitle('Learning Curves')
axarr = axarr.ravel()

for trained_model_data in trained_models_data:
    learning_rate, optimizer_name, model, history, test_loss, test_acc = trained_model_data
    label = f'lr={learning_rate}, optim={optimizer_name}'

    axarr[0].plot(history.history['loss'], label=f'train ({label})')
    axarr[0].set_title('Loss vs Epoch (Train set)')
    axarr[0].set(xlabel='Epoch', ylabel='Loss')
    axarr[0].legend()

    axarr[1].plot(history.history['accuracy'], label=f'train ({label})')
    axarr[1].set_title('Accuracy vs Epoch (Train set)')
    axarr[1].set(xlabel='Epoch', ylabel='Accuracy')
    axarr[1].legend()

    axarr[2].plot(history.history['val_loss'], label=f'val ({label})')
    axarr[2].set_title('Loss vs Epoch (Val set)')
    axarr[2].set(xlabel='Epoch', ylabel='Loss')
    axarr[2].legend()

    axarr[3].plot(history.history['val_accuracy'], label=f'val ({label})')
    axarr[3].set_title('Accuracy vs Epoch (Val set)')
    axarr[3].set(xlabel='Epoch', ylabel='Accuracy')
    axarr[3].legend()

savefig_path = os.path.join(BASE_PATH, 'learning-curves.png')
fig.savefig(savefig_path, dpi=150)

fig, axarr = plt.subplots(nrows=1, ncols=2, figsize=(19.20, 7.20))
fig.suptitle('Learning Curves')
axarr = axarr.ravel()

for trained_model_data in trained_models_data:
    optimizer_name, learning_rate, model, history, test_loss, test_acc = trained_model_data
    label = f'lr={learning_rate}, optim={optimizer_name}'

    axarr[0].plot(history.history['loss'], label=f'train ({label})')
    axarr[0].plot(history.history['val_loss'], label=f'val ({label})')
    axarr[0].set_title('Loss vs Epoch')
    axarr[0].set(xlabel='Epoch', ylabel='Loss')
    axarr[0].legend()

    axarr[1].plot(history.history['accuracy'], label=f'train ({label})')
    axarr[1].plot(history.history['val_accuracy'], label=f'val ({label})')
    axarr[1].set_title('Accuracy vs Epoch')
    axarr[1].set(xlabel='Epoch', ylabel='Accuracy')
    axarr[1].legend()

# savefig_path = os.path.join(BASE_PATH, 'learning-curves.png')
# fig.savefig(savefig_path, dpi=150)

"""# Fine-tuning a Pre-trained model

Resize images to 32 by 32, and create 3 channels, this is the input shape required by VGG16 pre-trained model.
"""

def mnist_32x32(X_28x28):
    X_32x32 = []
    for image in X_28x28:
        resized_image = cv2.resize(image, dsize=(32, 32))
        resized_image = np.dstack([resized_image, resized_image, resized_image])
        X_32x32.append(resized_image)
    X_32x32 = np.array(X_32x32)
    return X_32x32

X_train_32x32 = mnist_32x32(X_train)
X_test_32x32 = mnist_32x32(X_test)
print('X_train_32x32:', X_train_32x32.shape)
print('X_test_32x32:', X_test_32x32.shape)

vgg16_backbone = vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(32, 32, 3), classifier_activation='softmax')
vgg16_backbone.summary()

vgg16_classifier = Sequential()
vgg16_classifier.add(vgg16_backbone)
vgg16_classifier.add(Flatten())
vgg16_classifier.add(Dense(512, activation='relu'))
vgg16_classifier.add(BatchNormalization())
vgg16_classifier.add(Dropout(0.3))
vgg16_classifier.add(Dense(128, activation='relu'))
vgg16_classifier.add(BatchNormalization())
vgg16_classifier.add(Dropout(0.3))
vgg16_classifier.add(Dense(n_classes, activation='softmax'))
vgg16_classifier.summary()

history, test_loss, test_acc = train_model(vgg16_classifier, 'adam', 0.001, X_train_32x32, y_train, X_test_32x32, y_test)

trained_models_data.append(['adam', 0.001, vgg16_classifier, history, test_loss, test_acc])

"""# Comparision between Custom Model and Fine-tuned VGG16"""

fig, axarr = plt.subplots(nrows=2, ncols=2, figsize=(16.0, 12.0))
fig.suptitle('Learning Curves')
axarr = axarr.ravel()

for idx, trained_model_data in enumerate(trained_models_data):
    optimizer_name, learning_rate, model, history, test_loss, test_acc = trained_model_data
    if learning_rate == 0.001:
        label = f'lr={learning_rate}, optim={optimizer_name}'
        if idx == len(trained_models_data) - 1:
            label = f'{label}, vgg16'
        else:
            label = f'{label}, custom'

        axarr[0].plot(history.history['loss'], label=f'train ({label})')
        axarr[0].set_title('Loss vs Epoch (Train set)')
        axarr[0].set(xlabel='Epoch', ylabel='Loss')
        axarr[0].legend()

        axarr[1].plot(history.history['accuracy'], label=f'train ({label})')
        axarr[1].set_title('Accuracy vs Epoch (Train set)')
        axarr[1].set(xlabel='Epoch', ylabel='Accuracy')
        axarr[1].legend()

        axarr[2].plot(history.history['val_loss'], label=f'val ({label})')
        axarr[2].set_title('Loss vs Epoch (Val set)')
        axarr[2].set(xlabel='Epoch', ylabel='Loss')
        axarr[2].legend()

        axarr[3].plot(history.history['val_accuracy'], label=f'val ({label})')
        axarr[3].set_title('Accuracy vs Epoch (Val set)')
        axarr[3].set(xlabel='Epoch', ylabel='Accuracy')
        axarr[3].legend()

savefig_path = os.path.join(BASE_PATH, 'learning-curves-comparision.png')
fig.savefig(savefig_path, dpi=150)

